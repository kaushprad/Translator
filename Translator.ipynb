{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3cc2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, GRU\n",
    "import string\n",
    "from string import digits\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9861f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = (r\"C:\\Users\\91880\\Desktop\\Intern\\dcep.eng.txt.en\")\n",
    "path2 = (r\"C:\\Users\\91880\\Desktop\\Intern\\dcep.en-fr_fr.txt.fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d984bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines= pd.read_table(path1, names=['eng'])\n",
    "lines2 = pd.read_table(path2,names =['fr'])\n",
    "lines = lines.join(lines2['fr'])\n",
    "lines = lines[0:10000]\n",
    "arr = lines.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebee5968",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed37925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf0c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "input_texts = []\n",
    "target_inputs = []\n",
    "target_outputs = []\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "     return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "for line in arr:\n",
    "    line[0]=re.sub(r\"[^a-zA-Z0-9]+\", ' ', str(line[0]))\n",
    "    line[1]=re.sub(r\"[^a-zA-Z0-9]+\", ' ', str(line[1]))\n",
    "    line[0] = re.sub(\"(\\d+)\",\"\",str(line[0]))\n",
    "    line[1] = re.sub(\"(\\d+)\",\"\",str(line[1]))\n",
    "    input_text = unicode_to_ascii(str(line[0]).lower().strip())\n",
    "    input_text = input_text.translate(str.maketrans('', '', string.punctuation))\n",
    "  \n",
    "    target_text = unicode_to_ascii(str(line[1]).lower().strip())\n",
    "    target_text = target_text.translate(str.maketrans('', '', string.punctuation))\n",
    "    target_input = \"<bof>\" + target_text \n",
    "    target_output = target_text+\"<eos>\"\n",
    "    input_texts.append(input_text)\n",
    "    target_inputs.append(target_input)\n",
    "    target_outputs.append(target_output)\n",
    "print(len(input_texts))\n",
    "print(len(target_outputs))\n",
    "print(len(target_inputs))\n",
    "print(input_texts[14])\n",
    "print(target_outputs[14])\n",
    "print(target_inputs[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eee61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "#convert sentences to word tokens and find length of vocab and max sentence length for input and output\n",
    "input_tokenizer = Tokenizer()\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "Source_vocabulary = input_tokenizer.word_index.keys()\n",
    "\n",
    "num_source_tokens = len(input_tokenizer.word_index)\n",
    "max_source_seq_length = max(len(text_to_word_sequence(x)) for x in input_texts)\n",
    "\n",
    "\n",
    "output_tokenizer = Tokenizer()\n",
    "output_tokenizer.fit_on_texts(target_inputs + target_outputs)\n",
    "target_vocabulary = output_tokenizer.word_index.keys()\n",
    "\n",
    "num_target_tokens = len(output_tokenizer.word_index)\n",
    "max_target_seq_length = max(len(text_to_word_sequence(y)) for y in target_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9265b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique source language tokens:', num_source_tokens)\n",
    "print('Number of unique target language tokens:', num_target_tokens)\n",
    "print('Max sequence length of source language:', max_source_seq_length)\n",
    "print('Max sequence length of target language:', max_target_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b2f3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert unique input words to interger values\n",
    "num_words_output = num_target_tokens + 1\n",
    "input_int = input_tokenizer.texts_to_sequences(input_texts)\n",
    "target_output_int = output_tokenizer.texts_to_sequences(target_outputs)\n",
    "target_input_int = output_tokenizer.texts_to_sequences(target_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b4edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad the sequences to the same length\n",
    "encoder_input = pad_sequences(input_int, maxlen=max_source_seq_length)\n",
    "print(\"endocder input shape\", encoder_input.shape)\n",
    "\n",
    "#add padding for encoder after the sentence integers\n",
    "decoder_input = pad_sequences(target_input_int, maxlen=max_target_seq_length, padding='post')\n",
    "print(\"decoder input shape\", decoder_input.shape)\n",
    "\n",
    "\n",
    "decoder_output = pad_sequences(target_output_int, maxlen=max_target_seq_length, padding='post')\n",
    "print(\"decoder output shape\", decoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd159d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embedding = dict()\n",
    "glove_file=\"C:\\\\Users\\\\91880\\\\Desktop\\\\Intern\\\\glove.6B.50d.txt\"\n",
    "\n",
    "g = open(glove_file , 'r',encoding=\"utf8\")\n",
    "\n",
    "\n",
    "for l in g:\n",
    "    vects = l.split()\n",
    "    w = vects[0]\n",
    "    vect_dim = asarray(vects[1:], dtype='float32')\n",
    "    embedding[w] = vect_dim\n",
    "\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e33dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words =  num_source_tokens+1\n",
    "embed_mat = zeros((words, 50))\n",
    "for value, position in input_tokenizer.word_index.items():\n",
    "    embedding_vect = embedding.get(value)\n",
    "    if embedding_vect is not None:\n",
    "        embed_mat[position] = embedding_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f172ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embed_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbd6158",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding['i'])\n",
    "print(embed_mat[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97550812",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09f7e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "with strategy.scope():\n",
    "    \n",
    "    layer_embedding = Embedding(words, 50, weights=[embed_mat], input_length=max_source_seq_length)\n",
    "    \n",
    "    \n",
    "    nodes_lstm = 256\n",
    "    epochs = 100\n",
    "    \n",
    "    \n",
    "    encoder_inputs = Input(shape=(max_source_seq_length,))\n",
    "    x = layer_embedding(encoder_inputs)\n",
    "    encoder_lstm = LSTM(nodes_lstm, return_state=True,return_sequences=True,dropout=0.4,recurrent_dropout=0)\n",
    "    encoder_outputs1, state_h, state_c = encoder_lstm(x)\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    decoder_inputs = Input(shape=(max_target_seq_length-1,))\n",
    "    decoder_embedding = Embedding(num_words_output, nodes_lstm)\n",
    "    decoder_inputs_x = decoder_embedding(decoder_inputs)\n",
    " \n",
    "\n",
    "    decoder_lstm = LSTM(nodes_lstm, return_state=True,return_sequences=True,dropout=0.4,recurrent_dropout=0)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)\n",
    "\n",
    "\n",
    "    decoder_dense = Dense(num_words_output, activation='sigmoid')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "\n",
    "\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    model.compile(optimizer='rmsprop',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81decda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf9b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split 85:15 for training and test for each layer training data inputted into model fit\n",
    "from sklearn.model_selection import train_test_split\n",
    "encoder_train, encoder_test, decoder_train, decoder_test,decoder_targets_train, decoder_targets_test = train_test_split(encoder_input, decoder_input,decoder_output, test_size=0.15, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a236e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_dataset = tf.data.Dataset.from_tensor_slices((encoder_train, decoder_train[:,:-1]))\n",
    "test_tf_dataset = tf.data.Dataset.from_tensor_slices((encoder_test, decoder_test[:,:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65abe680",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_dataset_1 = tf.data.Dataset.from_tensor_slices(decoder_train.reshape(decoder_train.shape[0],decoder_train.shape[1], 1)[:,1:])\n",
    "test_tf_dataset_1 = tf.data.Dataset.from_tensor_slices(decoder_test.reshape(decoder_test.shape[0],decoder_test.shape[1], 1)[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea729539",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoder_test[0])\n",
    "print(decoder_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26a4cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.zip(((train_tf_dataset, train_tf_dataset_1)))\n",
    "\n",
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2a29ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.zip(((test_tf_dataset, test_tf_dataset_1)))\n",
    "\n",
    "test_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7d0cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502d217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff48172",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3147e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb094f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "filepath = \"intern_ml-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "es = EarlyStopping(monitor = 'val_loss'  ,mode='min' , verbose=1 , patience=2)\n",
    "model.fit(train_dataset,validation_data=test_dataset, epochs=20 ,verbose = 1,callbacks=[es] , batch_size = BATCH_SIZE)\n",
    "model.save('seq2seq_source_target.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092126a9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871722e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('seq2seq_source_target.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328fc95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update encoder and decoder models for testing to allow predictions word by word\n",
    "#encoder stays the same\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_h = Input(shape=(nodes_lstm,))\n",
    "decoder_c = Input(shape=(nodes_lstm,))\n",
    "decoder_states = [decoder_h, decoder_c]\n",
    "\n",
    "#shape is changed to single decoders rather than full phrase, can iterate through and translate sentence\n",
    "single_dec = Input(shape=(1,))\n",
    "single_dec_x = decoder_embedding(single_dec)\n",
    "decoder_out, h, c = decoder_lstm(single_dec_x, initial_state=decoder_states)\n",
    "\n",
    "decode = [h, c]\n",
    "decoder_out = decoder_dense(decoder_out)\n",
    "\n",
    "decoder_model = Model([single_dec] + decoder_states,[decoder_out] + decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215f1cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert integers back to words for source and target\n",
    "#create dictionaries to convert them back\n",
    "word_input = {value:key for key, value in input_tokenizer.word_index.items()}\n",
    "word_target = {value:key for key, value in output_tokenizer.word_index.items()}\n",
    "\n",
    "#method used to translate, converting the integer values of each sentence back to the word vectors\n",
    "def translator(input):\n",
    "    states = encoder_model.predict(input)\n",
    "    target = np.zeros((1, 1))\n",
    "    \n",
    "    target[0, 0] = output_tokenizer.word_index['bof']\n",
    "    eos = output_tokenizer.word_index['eos']\n",
    "    out_sen = []\n",
    "\n",
    "    for _ in range(max_target_seq_length):\n",
    "        out_tok, h, c = decoder_model.predict([target] + states)\n",
    "        index = np.argmax(out_tok[0, 0, :])\n",
    "\n",
    "        if eos == index:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "\n",
    "        if index > 0:\n",
    "            word = word_target[index]\n",
    "            out_sen.append(word)\n",
    "\n",
    "        target[0, 0] = index\n",
    "        states = [h, c]\n",
    "\n",
    "    return ' '.join(out_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df3f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLEU evalution of test dataset\n",
    "\n",
    "import nltk\n",
    "from statistics import mean \n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "smoothie = SmoothingFunction().method4\n",
    "accuracy_overall = []\n",
    "#iterate through all test set data and predict french translation\n",
    "#record the accuracy for each sentence and add to list, get the mean value of this list \n",
    "#returns the accuracy of test data\n",
    "for sentence in range (20,51):\n",
    "    input_sequence = encoder_input[sentence:sentence+1]\n",
    "    translate = translator(input_sequence)\n",
    "    #   print('English', input_texts[sentence])\n",
    "    #   print('French', translate)\n",
    "    #   print(\"------------------------------------------------\")\n",
    "\n",
    "\n",
    "    actual = target_outputs[sentence].split()\n",
    "    predict = translate.split()\n",
    "    actual = actual[:-1]\n",
    "          #print(actual,predict)\n",
    "\n",
    "          #use BLEU to score the actual vs predicted sets\n",
    "    accuracy = nltk.translate.bleu_score.sentence_bleu([actual],predict , smoothing_function = smoothie)\n",
    "    if len(actual)>=4:\n",
    "        accuracy_overall.append(accuracy)\n",
    "\n",
    "#return the accuracy from the average across all translations\n",
    "test_accuracy = round(mean(accuracy_overall),2)\n",
    "print(\"Test Score is: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03cd3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = round(mean(accuracy_overall),2)\n",
    "print(\"Test Score is: \", test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
